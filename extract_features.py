"""
Feature Extraction Script for Transformer Brain Encoder
========================================================

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ç”»åƒã‹ã‚‰ç‰¹å¾´é‡ã‚’äº‹å‰ã«æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã™ã€‚
This script pre-extracts image features and saves them for later use.

ä½¿ç”¨æ–¹æ³• (Usage):
    python extract_features.py \
        --data_dir /path/to/algonauts_data/subj01 \
        --output_dir /path/to/save/features \
        --subj 01 \
        --backbone dinov2_q \
        --batch_size 16

ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ (Supported backbones):
    - dinov2_q: DINOv2 with QKV hooks (æ¨å¥¨/recommended)
    - dinov2: DINOv2 standard
    - clip: CLIP ViT-L-14

å‡ºåŠ›å½¢å¼ (Output format):
    saved_feats_dir/
    â”œâ”€â”€ dinov2_q_last/
    â”‚   â””â”€â”€ {subj}/
    â”‚       â”œâ”€â”€ train.npy    # [N_train, num_patches+1, 768]
    â”‚       â””â”€â”€ synt.npy     # [N_test, num_patches+1, 768]
    â””â”€â”€ clip_vit_512/
        â””â”€â”€ {subj}/
            â”œâ”€â”€ train.npy    # [N_train, num_patches+1, 512]
            â””â”€â”€ synt.npy     # [N_test, num_patches+1, 512]
"""

import os
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import torchvision.transforms as transforms
import argparse

from models.dino import dino_model_with_hooks, dino_model
from models.clip import clip_model
from utils.utils import NestedTensor


def extract_dino_features_with_hooks(image_dir, output_path, enc_output_layer=-1, batch_size=16, device='cuda'):
    """
    DINOv2 with hooks ã‚’ä½¿ã£ãŸç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰
    QKVç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€datasets/nsd.pyã¨äº’æ›æ€§ã®ã‚ã‚‹å½¢å¼ã§ä¿å­˜
    
    ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–:
    - numpy.memmapã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã‚’æ®µéšçš„ã«æ›¸ãè¾¼ã‚€
    - ãƒãƒƒãƒå‡¦ç†å¾Œã«torch.cuda.empty_cache()ã‚’å‘¼ã³å‡ºã—ã¦GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
    
    Args:
        image_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.npy)
        enc_output_layer: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å±¤ã®æŒ‡å®š (-1=æœ€çµ‚å±¤)
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        device: ãƒ‡ãƒã‚¤ã‚¹ ('cuda' or 'cpu')
    
    Returns:
        all_features: numpy array [num_images, num_patches+1, 768]
    """
    
    print(f"ğŸ”§ Extracting DINO features (with hooks) from {image_dir}")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model = dino_model_with_hooks(enc_output_layer=enc_output_layer, 
                                  return_interm_layers=False,
                                  return_cls=False)
    model = model.to(device)
    model.eval()
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    img_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])
    num_images = len(img_files)
    print(f"Found {num_images} images")
    
    # æ­£è¦åŒ– (datasets/nsd.pyã¨åŒã˜)
    normalize = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    patch_size = 14
    
    # ç‰¹å¾´é‡ã®å½¢çŠ¶ã‚’æ±ºå®š (DINOv2: 962ãƒ‘ãƒƒãƒ + 768æ¬¡å…ƒ)
    num_patches = 962  # 31*31 + 1 CLS token
    feature_dim = 768
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—é…åˆ—ã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
    memmap_features = np.memmap(output_path + '.tmp', dtype='float32', mode='w+', 
                                shape=(num_images, num_patches, feature_dim))
    
    # ãƒãƒƒãƒå‡¦ç†ã§ç‰¹å¾´æŠ½å‡º
    current_idx = 0
    for i in tqdm(range(0, len(img_files), batch_size), desc="Extracting features"):
        batch_files = img_files[i:i+batch_size]
        batch_imgs = []
        
        for img_file in batch_files:
            img_path = os.path.join(image_dir, img_file)
            img = Image.open(img_path).convert('RGB')
            img = img.resize((224, 224))
            img_tensor = normalize(img)
            
            # DINOv2ç”¨ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° (datasets/nsd.pyã¨åŒã˜å‡¦ç†)
            size_im = (
                img_tensor.shape[0],
                int(np.ceil(img_tensor.shape[1] / patch_size) * patch_size),
                int(np.ceil(img_tensor.shape[2] / patch_size) * patch_size),
            )
            paded = torch.zeros(size_im)
            paded[:, :img_tensor.shape[1], :img_tensor.shape[2]] = img_tensor
            batch_imgs.append(paded)
        
        # ãƒãƒƒãƒã‚’ä½œæˆ
        batch_tensor = torch.stack(batch_imgs).to(device)
        
        # NestedTensor ã‚’ä½œæˆ
        mask = torch.ones(batch_tensor.shape[0], 
                         batch_tensor.shape[2], 
                         batch_tensor.shape[3], 
                         dtype=torch.bool, device=device)
        nested_tensor = NestedTensor(batch_tensor, mask)
        
        # ç‰¹å¾´æŠ½å‡º (models/dino.pyã®forwardå‡¦ç†ã‚’å†ç¾)
        with torch.no_grad():
            xs = nested_tensor.tensors
            h, w = int(xs.shape[2]/14), int(xs.shape[3]/14)
            
            # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‹ã‚‰ä¸­é–“å±¤ã‚’å–å¾—
            xs = model.backbone.get_intermediate_layers(xs)[0]
            
            # QKVç‰¹å¾´ã‚’å–å¾— (hookçµŒç”±)
            feats = model.qkv_feats['qkv_feats']
            
            # Reshape (models/dino.py ã®58-62è¡Œç›®ã¨åŒã˜)
            nh = 12  # Number of heads
            feats = feats.reshape(xs.shape[0], xs.shape[1]+1, 3, nh, -1 // nh).permute(2, 0, 3, 1, 4)
            q, k, v = feats[0], feats[1], feats[2]
            q = q.transpose(1, 2).reshape(xs.shape[0], xs.shape[1]+1, -1)
            
            # xs = q[:,1:,:] ã¨ã—ã¦CLSãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–
            xs_feats = q[:,1:,:]  # [B, 961, 768] (31x31ãƒ‘ãƒƒãƒ)
            
            # ãŸã ã—ã€datasets/nsd.pyã§ã¯ img[1:,:] ã‚’ä½¿ç”¨ã—ã¦CLSãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ã™ã‚‹ãŸã‚
            # ã“ã“ã§ã¯CLSãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚ãŸã¾ã¾ä¿å­˜ã™ã‚‹
            # ã¤ã¾ã‚Š q å…¨ä½“ã‚’ä¿å­˜: [B, 962, 768] (961ãƒ‘ãƒƒãƒ + 1 CLS)
            feats_with_cls = q  # [B, 962, 768]
            
            feats_np = feats_with_cls.cpu().numpy()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã«ç›´æ¥æ›¸ãè¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
        batch_size_actual = len(batch_files)
        memmap_features[current_idx:current_idx+batch_size_actual] = feats_np
        current_idx += batch_size_actual
        
        # GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
        if device == 'cuda':
            torch.cuda.empty_cache()
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
    memmap_features.flush()
    
    print(f"âœ… Feature shape: {memmap_features.shape}")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•
    import shutil
    shutil.move(output_path + '.tmp', output_path)
    print(f"âœ… Saved to {output_path}")
    
    # ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’è¿”ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
    all_features = np.load(output_path, mmap_mode='r')
    return all_features


def extract_dino_features_simple(image_dir, output_path, enc_output_layer=-1, batch_size=16, device='cuda'):
    """
    é€šå¸¸ã® DINO ã‚’ä½¿ã£ãŸç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰
    
    ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–:
    - numpy.memmapã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã‚’æ®µéšçš„ã«æ›¸ãè¾¼ã‚€
    - ãƒãƒƒãƒå‡¦ç†å¾Œã«torch.cuda.empty_cache()ã‚’å‘¼ã³å‡ºã—ã¦GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
    
    Args:
        image_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.npy)
        enc_output_layer: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å±¤ã®æŒ‡å®š (-1=æœ€çµ‚å±¤)
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        device: ãƒ‡ãƒã‚¤ã‚¹ ('cuda' or 'cpu')
    
    Returns:
        all_features: numpy array [num_images, num_patches+1, 768]
    """
    
    print(f"ğŸ”§ Extracting DINO features (simple) from {image_dir}")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model = dino_model(enc_output_layer=enc_output_layer, 
                      return_interm_layers=False,
                      return_cls=False)
    model = model.to(device)
    model.eval()
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    img_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])
    num_images = len(img_files)
    print(f"Found {num_images} images")
    
    # æ­£è¦åŒ–
    normalize = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    patch_size = 14
    
    # ç‰¹å¾´é‡ã®å½¢çŠ¶ã‚’æ±ºå®š (DINOv2: 962ãƒ‘ãƒƒãƒ + 768æ¬¡å…ƒ)
    num_patches = 962  # 31*31 + 1 CLS token
    feature_dim = 768
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—é…åˆ—ã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
    memmap_features = np.memmap(output_path + '.tmp', dtype='float32', mode='w+', 
                                shape=(num_images, num_patches, feature_dim))
    
    # ãƒãƒƒãƒå‡¦ç†ã§ç‰¹å¾´æŠ½å‡º
    current_idx = 0
    for i in tqdm(range(0, len(img_files), batch_size), desc="Extracting features"):
        batch_files = img_files[i:i+batch_size]
        batch_imgs = []
        
        for img_file in batch_files:
            img_path = os.path.join(image_dir, img_file)
            img = Image.open(img_path).convert('RGB')
            img = img.resize((224, 224))
            img_tensor = normalize(img)
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            size_im = (
                img_tensor.shape[0],
                int(np.ceil(img_tensor.shape[1] / patch_size) * patch_size),
                int(np.ceil(img_tensor.shape[2] / patch_size) * patch_size),
            )
            paded = torch.zeros(size_im)
            paded[:, :img_tensor.shape[1], :img_tensor.shape[2]] = img_tensor
            batch_imgs.append(paded)
        
        # ãƒãƒƒãƒã‚’ä½œæˆ
        batch_tensor = torch.stack(batch_imgs).to(device)
        
        # NestedTensor ã‚’ä½œæˆ
        mask = torch.ones(batch_tensor.shape[0], 
                         batch_tensor.shape[2], 
                         batch_tensor.shape[3], 
                         dtype=torch.bool, device=device)
        nested_tensor = NestedTensor(batch_tensor, mask)
        
        # ç‰¹å¾´æŠ½å‡º
        with torch.no_grad():
            xs = nested_tensor.tensors
            patch_size = 14
            w_p = int(xs.shape[2] / patch_size)
            h_p = int(xs.shape[3] / patch_size)
            
            xs = model.backbone.get_intermediate_layers(xs, n=12)
            xs_layer = xs[enc_output_layer]  # [B, num_patches+1, 768]
            
            # CLSãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€å½¢å¼ã§ä¿å­˜
            # datasets/nsd.pyãŒreshapeã«ä½¿ç”¨ã™ã‚‹
            feats_np = xs_layer.cpu().numpy()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã«ç›´æ¥æ›¸ãè¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
        batch_size_actual = len(batch_files)
        memmap_features[current_idx:current_idx+batch_size_actual] = feats_np
        current_idx += batch_size_actual
        
        # GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
        if device == 'cuda':
            torch.cuda.empty_cache()
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
    memmap_features.flush()
    
    print(f"âœ… Feature shape: {memmap_features.shape}")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•
    import shutil
    shutil.move(output_path + '.tmp', output_path)
    print(f"âœ… Saved to {output_path}")
    
    # ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’è¿”ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
    all_features = np.load(output_path, mmap_mode='r')
    return all_features


def extract_clip_features(image_dir, output_path, enc_output_layer=-1, batch_size=16, device='cuda'):
    """
    CLIP ã‚’ä½¿ã£ãŸç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰
    
    ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–:
    - numpy.memmapã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã‚’æ®µéšçš„ã«æ›¸ãè¾¼ã‚€
    - ãƒãƒƒãƒå‡¦ç†å¾Œã«torch.cuda.empty_cache()ã‚’å‘¼ã³å‡ºã—ã¦GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
    
    Args:
        image_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.npy)
        enc_output_layer: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å±¤ã®æŒ‡å®š (CLIPã§ã¯æœªä½¿ç”¨)
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        device: ãƒ‡ãƒã‚¤ã‚¹ ('cuda' or 'cpu')
    
    Returns:
        all_features: numpy array [num_images, num_patches+1, 768]
    """
    
    print(f"ğŸ”§ Extracting CLIP features from {image_dir}")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model = clip_model(enc_output_layer=enc_output_layer, 
                      return_interm_layers=False,
                      return_cls=False)
    model = model.to(device)
    model.eval()
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    img_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])
    num_images = len(img_files)
    print(f"Found {num_images} images")
    
    # æ­£è¦åŒ–
    normalize = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # ç‰¹å¾´é‡ã®å½¢çŠ¶ã‚’æ±ºå®š (CLIP: 257ãƒ‘ãƒƒãƒ + 768æ¬¡å…ƒ)
    num_patches = 257  # 16*16 + 1 CLS token
    feature_dim = 768
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—é…åˆ—ã‚’ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
    memmap_features = np.memmap(output_path + '.tmp', dtype='float32', mode='w+', 
                                shape=(num_images, num_patches, feature_dim))
    
    # ãƒãƒƒãƒå‡¦ç†ã§ç‰¹å¾´æŠ½å‡º
    current_idx = 0
    for i in tqdm(range(0, len(img_files), batch_size), desc="Extracting features"):
        batch_files = img_files[i:i+batch_size]
        batch_imgs = []
        
        for img_file in batch_files:
            img_path = os.path.join(image_dir, img_file)
            img = Image.open(img_path).convert('RGB')
            img = img.resize((224, 224))
            img_tensor = normalize(img)
            batch_imgs.append(img_tensor)
        
        # ãƒãƒƒãƒã‚’ä½œæˆ
        batch_tensor = torch.stack(batch_imgs).to(device)
        
        # ç‰¹å¾´æŠ½å‡º (models/clip.pyã®forwardå‡¦ç†ã‚’å†ç¾)
        with torch.no_grad():
            # CLIP visual encoder
            cls_token, patch_tokens = model.backbone.visual(batch_tensor)
            
            # Project patch tokens from 1024 â†’ 768
            proj = model.backbone.visual.proj  # shape: (1024, 768)
            patch_tokens_proj = patch_tokens @ proj  # (B, 256, 768)
            
            # CLSãƒˆãƒ¼ã‚¯ãƒ³ã‚‚å«ã‚ã‚‹å½¢å¼ã§ä¿å­˜
            cls_token_reshaped = cls_token.unsqueeze(1)  # (B, 1, 768)
            full_tokens = torch.cat([cls_token_reshaped, patch_tokens_proj], dim=1)  # (B, 257, 768)
            
            feats_np = full_tokens.cpu().numpy()
        
        # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã«ç›´æ¥æ›¸ãè¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
        batch_size_actual = len(batch_files)
        memmap_features[current_idx:current_idx+batch_size_actual] = feats_np
        current_idx += batch_size_actual
        
        # GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰
        if device == 'cuda':
            torch.cuda.empty_cache()
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
    memmap_features.flush()
    
    print(f"âœ… Feature shape: {memmap_features.shape}")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ç§»å‹•
    import shutil
    shutil.move(output_path + '.tmp', output_path)
    print(f"âœ… Saved to {output_path}")
    
    # ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’è¿”ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
    all_features = np.load(output_path, mmap_mode='r')
    return all_features


def main():
    parser = argparse.ArgumentParser(
        description='Extract image features for Transformer Brain Encoder',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument('--data_dir', type=str, required=True,
                       help='Path to algonauts data directory (e.g., /path/to/subj01)')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='Output directory root for saving features')
    parser.add_argument('--subj', type=str, default='01',
                       help='Subject ID (e.g., 01, 02, ...)')
    parser.add_argument('--backbone', type=str, default='dinov2_q',
                       choices=['dinov2_q', 'dinov2', 'clip'],
                       help='Backbone model to use for feature extraction')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size for feature extraction')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda or cpu)')
    parser.add_argument('--enc_layer', type=int, default=-1,
                       help='Encoder layer to extract features from (-1 = last layer)')
    
    args = parser.parse_args()
    
    print("="*60)
    print("Feature Extraction Configuration")
    print("="*60)
    print(f"Data directory: {args.data_dir}")
    print(f"Output directory: {args.output_dir}")
    print(f"Subject: {args.subj}")
    print(f"Backbone: {args.backbone}")
    print(f"Batch size: {args.batch_size}")
    print(f"Device: {args.device}")
    print(f"Encoder layer: {args.enc_layer}")
    print("="*60)
    
    # Check device availability
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDA not available, using CPU instead")
        args.device = 'cpu'
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
    train_img_dir = os.path.join(args.data_dir, 'training_split', 'training_images')
    test_img_dir = os.path.join(args.data_dir, 'test_split', 'test_images')
    
    if not os.path.exists(train_img_dir):
        print(f"âŒ Training image directory not found: {train_img_dir}")
        return
    
    if not os.path.exists(test_img_dir):
        print(f"âš ï¸  Test image directory not found: {test_img_dir}")
        print("   Skipping test feature extraction")
        test_img_dir = None
    
    # å‡ºåŠ›ãƒ‘ã‚¹è¨­å®š
    if args.backbone == 'dinov2_q':
        output_subdir = 'dinov2_q_last'
    elif args.backbone == 'dinov2':
        output_subdir = 'dinov2_last'
    elif args.backbone == 'clip':
        output_subdir = 'clip_vit_512'
    
    output_subject_dir = os.path.join(args.output_dir, output_subdir, args.subj)
    train_output_path = os.path.join(output_subject_dir, 'train.npy')
    test_output_path = os.path.join(output_subject_dir, 'synt.npy')
    
    # ç‰¹å¾´æŠ½å‡ºé–¢æ•°ã®é¸æŠ
    if args.backbone == 'dinov2_q':
        extract_fn = extract_dino_features_with_hooks
    elif args.backbone == 'dinov2':
        extract_fn = extract_dino_features_simple
    elif args.backbone == 'clip':
        extract_fn = extract_clip_features
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´æŠ½å‡º
    print("\n" + "="*60)
    print("Extracting TRAINING features")
    print("="*60)
    train_features = extract_fn(
        train_img_dir, 
        train_output_path,
        enc_output_layer=args.enc_layer,
        batch_size=args.batch_size,
        device=args.device
    )
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´æŠ½å‡º
    if test_img_dir:
        print("\n" + "="*60)
        print("Extracting TEST features")
        print("="*60)
        test_features = extract_fn(
            test_img_dir,
            test_output_path,
            enc_output_layer=args.enc_layer,
            batch_size=args.batch_size,
            device=args.device
        )
    
    print("\n" + "="*60)
    print("âœ… Feature extraction completed successfully!")
    print("="*60)
    print(f"\nSaved features to:")
    print(f"  Training: {train_output_path}")
    if test_img_dir:
        print(f"  Test:     {test_output_path}")
    
    print(f"\nTo use these features in training, run:")
    print(f"  python main.py \\")
    print(f"    --subj {int(args.subj)} \\")
    if args.backbone == 'dinov2_q':
        print(f"    --saved_feats dinov2q \\")
    elif args.backbone == 'clip':
        print(f"    --saved_feats clip \\")
    print(f"    --saved_feats_dir {args.output_dir} \\")
    print(f"    --encoder_arch transformer \\")
    print(f"    --readout_res rois_all")
    print("="*60)


if __name__ == '__main__':
    main()
