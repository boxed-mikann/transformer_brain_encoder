# extract_features.pyï¼ˆãƒªãƒã‚¸ãƒˆãƒªæº–æ‹ ç‰ˆï¼‰
import os
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import torchvision.transforms as transforms
import argparse
import sys

# ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models.dino import dino_model_with_hooks
from models.clip import clip_model
from utils.utils import NestedTensor

def extract_features(image_dir, output_dir, model_type='dino', batch_size=16, device='cuda'):
    """
    ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã«æº–æ‹ ã—ãŸç‰¹å¾´æŠ½å‡º
    datasets/nsd.py ã®æ§‹é€ ã«åˆã‚ã›ãŸå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    """
    
    print(f"ğŸ”§ Extracting {model_type.upper()} features from {image_dir}")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã«æº–æ‹ ï¼‰
    if model_type == 'dino':
        model = dino_model_with_hooks(enc_output_layer=-1, return_interm_layers=False)
    elif model_type == 'clip':
        model = clip_model(enc_output_layer=-1, return_interm_layers=False)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    model = model.to(device)
    model.eval()
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    img_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])
    print(f"Found {len(img_files)} images")
    
    # æ­£è¦åŒ–
    normalize = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    all_features = []
    
    # ãƒãƒƒãƒå‡¦ç†ã§ç‰¹å¾´æŠ½å‡º
    for i in tqdm(range(0, len(img_files), batch_size)):
        batch_files = img_files[i:i+batch_size]
        batch_imgs = []
        
        for img_file in batch_files:
            img_path = os.path.join(image_dir, img_file)
            img = Image.open(img_path).convert('RGB')
            img = img.resize((224, 224))
            img_tensor = normalize(img)
            batch_imgs.append(img_tensor)
        
        # ãƒãƒƒãƒã‚’ä½œæˆ
        batch_tensor = torch.stack(batch_imgs).to(device)
        
        # NestedTensor ã‚’ä½œæˆï¼ˆdatasets/nsd.py ã§ä½¿ç”¨ã•ã‚Œã‚‹ã®ã¨åŒã˜å½¢å¼ï¼‰
        mask = torch.ones(batch_tensor.shape[0], 
                         batch_tensor.shape[2], 
                         batch_tensor.shape[3], 
                         dtype=torch.bool, device=device)
        nested_tensor = NestedTensor(batch_tensor, mask)
        
        # ç‰¹å¾´æŠ½å‡º
        with torch.no_grad():
            outputs = model(nested_tensor)
        
        # datasets/nsd.py ã®62è¡Œç›®ã«åˆã‚ã›ãŸå½¢å¼ã«å¤‰æ›
        # img = torch.reshape(img, (962, 768))  â† ã“ã‚Œã«å¯¾å¿œã™ã‚‹ã‚µã‚¤ã‚º
        
        feats = outputs['layer_top'].tensors  # (batch, 768, h, w)
        
        # å½¢çŠ¶ã‚’èª¿æ•´ï¼ˆdatasets/nsd.py ã§ã® reshape ã«å¯¾å¿œï¼‰
        batch_size_actual = feats.shape[0]
        feats_flat = feats.reshape(batch_size_actual, feats.shape[1], -1)  # (batch, 768, h*w)
        feats_flat = feats_flat.permute(0, 2, 1)  # (batch, h*w, 768)
        feats_flat = feats_flat.cpu().numpy()
        
        all_features.append(feats_flat)
    
    # ã™ã¹ã¦ã®ç‰¹å¾´ã‚’çµåˆ
    all_features = np.concatenate(all_features, axis=0)
    print(f"Feature shape: {all_features.shape}")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ï¼ˆdatasets/nsd.py ã®21-22è¡Œç›®ã«åˆã‚ã›ãŸå‘½åï¼‰
    output_path = os.path.join(output_dir, 'train.npy')
    np.save(output_path, all_features)
    print(f"âœ… Saved to {output_path}")
    
    return all_features


def main():
    parser = argparse.ArgumentParser(description='Extract image features (compatible with transformer_brain_encoder)')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='Path to algonauts data (e.g., /path/to/subj01)')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='Output directory root (e.g., /path/to/image_features)')
    parser.add_argument('--subj', type=str, default='01',
                       help='Subject ID (e.g., 01, 02, ...)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size for feature extraction')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device (cuda or cpu)')
    
    args = parser.parse_args()
    
    # è¨“ç·´ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    train_img_dir = os.path.join(args.data_dir, 'training_split', 'training_images')
    
    if not os.path.exists(train_img_dir):
        print(f"âŒ Image directory not found: {train_img_dir}")
        return
    
    # DINO ç‰¹å¾´æŠ½å‡º
    # datasets/nsd.py ã®21è¡Œç›®: dino_feat_dir = args.saved_feats_dir + '/dinov2_q_last/'+ args.subj
    dino_output_dir = os.path.join(args.output_dir, 'dinov2_q_last', args.subj)
    extract_features(train_img_dir, dino_output_dir, 
                    model_type='dino',
                    batch_size=args.batch_size,
                    device=args.device)
    
    # CLIP ç‰¹å¾´æŠ½å‡º
    # datasets/nsd.py ã®22è¡Œç›®: clip_feat_dir = args.saved_feats_dir + '/clip_vit_512/'+ args.subj
    clip_output_dir = os.path.join(args.output_dir, 'clip_vit_512', args.subj)
    extract_features(train_img_dir, clip_output_dir,
                    model_type='clip',
                    batch_size=args.batch_size,
                    device=args.device)
    
    print("âœ… Feature extraction complete!")


if __name__ == '__main__':
    main()
